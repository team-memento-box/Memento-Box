import json
import base64
import os
import tiktoken
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from dotenv import load_dotenv
from openai import AzureOpenAI
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@dataclass
class StrangeResponse:
    """ì´ìƒí•œ ë‹µë³€ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    question: str
    answer: str
    timestamp: str
    severity: str  # "mild", "moderate", "severe"

@dataclass
class ConversationTurn:
    """ëŒ€í™” í„´ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    question: str
    answer: str
    timestamp: str

class ImageAnalysisGPT:
    """GPT-4oë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ í´ë˜ìŠ¤"""
    def __init__(self):
        # Azure OpenAI ê´€ë ¨ ì„¤ì •
        self.endpoint = os.getenv("gpt-endpoint")
        self.deployment = "gpt-4o"
        self.subscription_key = os.getenv("gpt-key")
        self.api_version = "2024-12-01-preview"
        
        if not self.endpoint or not self.subscription_key:
            raise ValueError("Please set the gpt-endpoint and gpt-key in the environment variables.")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = AzureOpenAI(
            api_version=self.api_version,
            azure_endpoint=self.endpoint,
            api_key=self.subscription_key,
        )
    
    def encode_image_to_base64(self, image_path):
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except FileNotFoundError:
            print(f"Error: ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_image_with_gpt(self, image_path):
        """GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        base64_image = self.encode_image_to_base64(image_path)
        if not base64_image:
            return None
        
        try:
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:

1. caption: ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ ì„¤ëª… (êµ¬ì²´ì ìœ¼ë¡œ ê·¸ë¦¬ê³  í•œí¸ì˜ ì´ì•¼ê¸°ì²˜ëŸ¼)
2. dense_captions: ì´ë¯¸ì§€ì˜ ì„¸ë¶€ì ì¸ ìš”ì†Œë“¤ì„ ì—¬ëŸ¬ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª… (ë°°ì—´ í˜•íƒœ)
3. mood: ì´ë¯¸ì§€ì—ì„œ ëŠê»´ì§€ëŠ” ë¶„ìœ„ê¸°ë‚˜ ê°ì •
4. time_period: ì¶”ì •ë˜ëŠ” ì‹œëŒ€ë‚˜ ì‹œê¸°
5. key_objects: ì£¼ìš” ê°ì²´ë“¤ (ë°°ì—´ í˜•íƒœ)
6. people_description: ì‚¬ëŒì´ ìˆë‹¤ë©´ ê·¸ë“¤ì— ëŒ€í•œ ì„¤ëª…

ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”:
{
    "caption": "ì „ì²´ ì´ë¯¸ì§€ ì„¤ëª…",
    "dense_captions": ["ì„¸ë¶€ì‚¬í•­1", "ì„¸ë¶€ì‚¬í•­2", "ì„¸ë¶€ì‚¬í•­3"],
    "mood": "ë¶„ìœ„ê¸° ì„¤ëª…",
    "time_period": "ì¶”ì • ì‹œëŒ€",
    "key_objects": ["ê°ì²´1", "ê°ì²´2", "ê°ì²´3"],
    "people_description": "ì‚¬ëŒë“¤ì— ëŒ€í•œ ì„¤ëª…"
}"""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            # ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
            response_text = response.choices[0].message.content
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```jsonìœ¼ë¡œ ê°ì‹¸ì ¸ ìˆì„ ìˆ˜ ìˆìŒ)
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "{" in response_text:
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                response_text = response_text[json_start:json_end]
            
            analysis_result = json.loads(response_text)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nCaption: {analysis_result.get('caption', 'N/A')}")
            print(f"Mood: {analysis_result.get('mood', 'N/A')}")
            print(f"Time Period: {analysis_result.get('time_period', 'N/A')}")
            print("\nDense Captions:")
            for caption in analysis_result.get('dense_captions', []):
                print(f"- {caption}")
            
            return analysis_result
            
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"ì›ë³¸ ì‘ë‹µ: {response_text}")
            return None
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None


class LLMDescription:
    def __init__(self):
        # Azure OpenAI ê´€ë ¨ ì„¤ì •
        self.endpoint = os.getenv("gpt-endpoint")
        self.deployment = "gpt-4o"
        self.subscription_key = os.getenv("gpt-key")
        self.api_version = "2024-12-01-preview"
        
        if not self.endpoint or not self.subscription_key:
            raise ValueError("Please set the gpt-endpoint and gpt-key in the environment variables.")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = AzureOpenAI(
            api_version=self.api_version,
            azure_endpoint=self.endpoint,
            api_key=self.subscription_key,
        )
        
        # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        self.conversation_history = []
        
        # í† í° ì¹´ìš´í„° ì´ˆê¸°í™”
        self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4oìš© í† í¬ë‚˜ì´ì €
        self.token_count = 0
        self.MAX_TOKENS = 4000  # ìµœëŒ€ í† í° ì œí•œ
        
        # ì´ìƒí•œ ë‹µë³€ ì¶”ì 
        self.strange_responses = []
        self.strange_response_count = 0
        self.last_question = ""  # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì €ì¥
        
        # ëŒ€í™” ê¸°ë¡ ì¶”ì  (íŒŒì¼ ì €ì¥ìš©)
        self.conversation_turns = []
        
    def _count_tokens(self, text: str) -> int:
        """ë¬¸ìì—´ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.tokenizer.encode(text))
    
    def _count_message_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """ëŒ€í™” ë©”ì‹œì§€ ëª©ë¡ì˜ ì´ í† í° ìˆ˜ ê³„ì‚°"""
        total = 0
        for message in messages:
            total += self._count_tokens(message.get("content", ""))
        return total
    
    def _evaluate_response_relevance(self, question: str, answer: str) -> Dict[str, Any]:
        """ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ LLMìœ¼ë¡œ í‰ê°€"""
        evaluation_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¶„ì„í•´ì„œ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì ì ˆí•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€: {answer}

í‰ê°€ ê¸°ì¤€:
1. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„±
2. ë‹µë³€ì˜ ì¼ê´€ì„±
3. ë§¥ë½ì  ì ì ˆì„±

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_strange": true/false,
    "severity": "normal/mild/moderate/severe",
    "reason": "í‰ê°€ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…"
}}

severity ê¸°ì¤€:
- normal: ì™„ì „íˆ ì ì ˆí•œ ë‹µë³€
- mild: ì•½ê°„ ë²—ì–´ë‚¬ì§€ë§Œ ì´í•´ ê°€ëŠ¥
- moderate: ìƒë‹¹íˆ ì—‰ëš±í•˜ì§€ë§Œ ì™„ì „íˆ ë¬´ê´€í•˜ì§€ëŠ” ì•ŠìŒ
- severe: ì™„ì „íˆ ë¬´ê´€í•˜ê±°ë‚˜ ë¹„ë…¼ë¦¬ì ì¸ ë‹µë³€
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ í™˜ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                max_tokens=256,
                temperature=0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                top_p=1.0,
            )
            
            evaluation_text = response.choices[0].message.content
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "```json" in evaluation_text:
                json_start = evaluation_text.find("```json") + 7
                json_end = evaluation_text.find("```", json_start)
                evaluation_text = evaluation_text[json_start:json_end].strip()
            elif "{" in evaluation_text:
                json_start = evaluation_text.find("{")
                json_end = evaluation_text.rfind("}") + 1
                evaluation_text = evaluation_text[json_start:json_end]
            
            evaluation_json = json.loads(evaluation_text)
            return evaluation_json
            
        except (json.JSONDecodeError, Exception) as e:
            print(f"ë‹µë³€ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "is_strange": False,
                "severity": "normal",
                "reason": "í‰ê°€ ì‹¤íŒ¨"
            }
    
    def _store_strange_response(self, question: str, answer: str, severity: str, reason: str):
        """ì´ìƒí•œ ë‹µë³€ì„ ì €ì¥ (ì½˜ì†” ì¶œë ¥ ì œê±°)"""
        strange_response = StrangeResponse(
            question=question,
            answer=answer,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            severity=severity
        )
        
        self.strange_responses.append(strange_response)
        self.strange_response_count += 1
        
    def setup_conversation_context(self, analysis_result, user_description="", user_description_date=""):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì„¤ì •"""
        caption = analysis_result.get("caption", "")
        dense_captions = analysis_result.get("dense_captions", [])
        mood = analysis_result.get("mood", "")
        time_period = analysis_result.get("time_period", "")
        key_objects = analysis_result.get("key_objects", [])
        people_description = analysis_result.get("people_description", "")
        
        # ìƒì„¸ ìº¡ì…˜ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        dense_captions_text = "\n".join([f"- {dc}" for dc in dense_captions])
        key_objects_text = ", ".join(key_objects)
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
        system_message = f"""ë‹¹ì‹ ì€ ì´ë¯¸ì§€ì— ëŒ€í•´ í• ë¨¸ë‹ˆ í• ì•„ë²„ì§€ì™€ ê°™ì´ ì–´ë¥´ì‹ ë“¤ê³¼ ëŒ€í™”í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

=== ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ===
ì£¼ìš” ì„¤ëª…(Caption): {caption}
ë¶„ìœ„ê¸°/ê°ì •: {mood}
ì¶”ì • ì‹œëŒ€: {time_period}
ì£¼ìš” ê°ì²´ë“¤: {key_objects_text}
ì¸ë¬¼ ì„¤ëª…: {people_description}

ì„¸ë¶€ ìš”ì†Œë“¤:
{dense_captions_text}

=== ëŒ€í™” ê°€ì´ë“œë¼ì¸ ===
1. ì–´ë¥´ì‹ ë“¤(íŠ¹íˆ ì¹˜ë§¤ í™˜ì)ê³¼ ëŒ€í™”í•œë‹¤ê³  ê°€ì •í•˜ê³  ì¹œê·¼í•˜ê³  ë”°ëœ»í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
2. ì´ë¯¸ì§€ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì„ ë¨¼ì € ë˜ì ¸ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
3. ì‚¬ìš©ìê°€ ì—‰ëš±í•œ ë‹µë³€ì„ í•´ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë©° ì¹œì ˆí•˜ê²Œ ì´ëŒì–´ì£¼ì„¸ìš”.
4. ê·¸ë•Œ ë‹¹ì‹œì˜ ê°ì •ì´ë‚˜ ê²½í—˜ì— ëŒ€í•´ ë¬¼ì–´ë³´ë©° ì¶”ì–µì„ ë˜ì‚´ë ¤ì£¼ì„¸ìš”.

=== ëŒ€í™” ì „ëµ ===
ë¬¸ì œ ìƒí™©ë³„ í•´ê²° ë°©ë²•:

â–ª ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í•  ê²½ìš°:
  - ì§ˆë¬¸ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì¬êµ¬ì„±
  - ì„ íƒì§€ë¥¼ ì œê³µí•˜ì—¬ ë‹µí•˜ê¸° ì‰½ê²Œ ë§Œë“¤ê¸°
  - ì˜ˆì‹œë‚˜ ë§¥ë½ì„ í•¨ê»˜ ì œê³µ

â–ª ì—‰ëš±í•œ ëŒ€ë‹µì„ í•  ê²½ìš°:
  - ëŒ€ë‹µì„ ìˆ˜ìš©í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì£¼ì œë¡œ ìœ ë„
  - ëŒ€ë‹µì˜ ì¼ë¶€ë¶„ì´ë¼ë„ ì—°ê²°ì ì„ ì°¾ì•„ ì´ì–´ê°€ê¸°

â–ª ëŒ€ë‹µì„ ëª»í•  ê²½ìš°:
  - ì‹¬ë¦¬ì  ë¶€ë‹´ ì—†ì´ ë„˜ì–´ê°€ê¸°
  - "ê¸°ì–µì´ ì•ˆ ë‚˜ì…”ë„ ê´œì°®ë‹¤"ê³  ì•ˆì‹¬ì‹œí‚¤ê¸°

ì´ë¯¸ì§€ì˜ ì‹œê°ì  ìš”ì†Œë“¤ì„ ìƒìƒí•˜ê²Œ ë¬˜ì‚¬í•˜ë©°, ê·¸ë•Œì˜ ê°ì •ê³¼ ìƒí™©ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ëŠ” ì†ì/ì†ë…€ì˜ ë§ˆìŒìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”."""
        
        # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë° í† í° ìˆ˜ ê³„ì‚°
        self.conversation_history = [{"role": "system", "content": system_message}]
        self.token_count = self._count_tokens(system_message)
        
        return system_message
    
    def generate_initial_question(self):
        """ì²« ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜"""
        response = self.client.chat.completions.create(
            model=self.deployment,
            messages=self.conversation_history + [
                {"role": "user", "content": "ì´ ì˜›ë‚  ì‚¬ì§„ì— ëŒ€í•´ ì–´ë¥´ì‹ ì—ê²Œ ë¬¼ì–´ë³¼ ì²« ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê°„ë‹¨í•˜ê³  ê¸°ì–µí•˜ê¸° ì‰¬ìš°ë©°, ê°ì •ì ìœ¼ë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤."}
            ],
            max_tokens=512,
            temperature=0.8,
            top_p=1.0,
        )
        
        initial_question = response.choices[0].message.content
        
        # ì§ˆë¬¸ ì¶”ê°€ ë° í† í° ìˆ˜ ì—…ë°ì´íŠ¸
        self.conversation_history.append({"role": "assistant", "content": initial_question})
        self.token_count += self._count_tokens(initial_question)
        
        # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì €ì¥
        self.last_question = initial_question
        
        return initial_question
    
    def chat_about_image(self, user_query):
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        # í† í° ì œí•œ í™•ì¸
        user_tokens = self._count_tokens(user_query)
        
        # ì‚¬ìš©ì ë‹µë³€ì˜ ì ì ˆì„± í‰ê°€ (ì´ì „ ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš°)
        if self.last_question:
            evaluation = self._evaluate_response_relevance(self.last_question, user_query)
            
            if evaluation.get("is_strange", False):
                severity = evaluation.get("severity", "mild")
                reason = evaluation.get("reason", "ê´€ë ¨ì„± ë¶€ì¡±")
                
                # ì´ìƒí•œ ë‹µë³€ ì €ì¥
                self._store_strange_response(
                    question=self.last_question,
                    answer=user_query,
                    severity=severity,
                    reason=reason
                )
        
        # ëŒ€í™” í„´ ì €ì¥ (ì§ˆë¬¸-ë‹µë³€ ìŒ)
        if self.last_question:
            conversation_turn = ConversationTurn(
                question=self.last_question,
                answer=user_query,
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            )
            self.conversation_turns.append(conversation_turn)
        
        # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        self.conversation_history.append({"role": "user", "content": user_query})
        self.token_count += user_tokens
        
        # í† í° ì œí•œ ì´ˆê³¼ í™•ì¸
        if self.token_count > self.MAX_TOKENS:
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì–˜ê¸°í•´ìš”. ì§€ê¸ˆì€ ì ì‹œ ì‰¬ì–´ì•¼ í•  ê²ƒ ê°™ì•„ìš”. ë§Œì•½ ë” ë§ì€ ëŒ€í™”ë¥¼ ì›í•œë‹¤ë©´ MEMENTO BOX Premiumì„ ì‚¬ìš©í•´ë³´ì„¸ìš”."
            self.conversation_history.append({"role": "assistant", "content": answer})
            return answer, True  # TrueëŠ” ëŒ€í™” ì¢…ë£Œ ì‹ í˜¸
        
        # LLM ì‘ë‹µ ìƒì„±
        response = self.client.chat.completions.create(
            model=self.deployment,
            messages=self.conversation_history,
            max_tokens=1024,
            temperature=0.7,
            top_p=1.0,
        )
        
        # ì‘ë‹µ ì¶”ì¶œ
        answer = response.choices[0].message.content
        
        # ì‘ë‹µ ì¶”ê°€ ë° í† í° ìˆ˜ ì—…ë°ì´íŠ¸
        self.conversation_history.append({"role": "assistant", "content": answer})
        self.token_count += self._count_tokens(answer)
        
        # ë‹¤ìŒ í‰ê°€ë¥¼ ìœ„í•´ í˜„ì¬ AI ì‘ë‹µì„ ì§ˆë¬¸ìœ¼ë¡œ ì €ì¥ (ì§ˆë¬¸ì´ í¬í•¨ëœ ê²½ìš°)
        if "?" in answer or "ê¹Œìš”" in answer or "ë‚˜ìš”" in answer:
            self.last_question = answer
        
        # í† í° ì œí•œ ì´ˆê³¼ í™•ì¸ (ì‘ë‹µ í›„)
        if self.token_count > self.MAX_TOKENS:
            return answer, True  # ëŒ€í™” ì¢…ë£Œ ì‹ í˜¸
        
        return answer, False  # ëŒ€í™” ê³„ì†
    
    def generate_mobile_report(self, image_path, output_dir="reports"):
        """ëª¨ë°”ì¼ í™”ë©´ì— ìµœì í™”ëœ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ìˆëŠ” í•œê¸€ í°íŠ¸ ì°¾ê¸°)
        try:
            # Windows
            font_path = "C:/Windows/Fonts/malgun.ttf"
            if not os.path.exists(font_path):
                # macOS
                font_path = "/System/Library/Fonts/AppleGothic.ttf"
                if not os.path.exists(font_path):
                    # Linux (Ubuntu)
                    font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
            
            if os.path.exists(font_path):
                font_prop = fm.FontProperties(fname=font_path)
                plt.rcParams['font.family'] = font_prop.get_name()
            else:
                # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
                plt.rcParams['font.family'] = 'DejaVu Sans'
        except:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        
        plt.rcParams['axes.unicode_minus'] = False
        
        # ë¦¬í¬íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ì „ì²´ ë‹µë³€ íšŸìˆ˜ ê³„ì‚°
        total_responses = len(self.conversation_turns)
        
        if total_responses == 0:
            print("ëŒ€í™”ê°€ ì§„í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        severity_counts = {"mild": 0, "moderate": 0, "severe": 0}
        for response in self.strange_responses:
            severity_counts[response.severity] += 1
        
        # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
        risk_score = (severity_counts['mild'] * 1 + 
                     severity_counts['moderate'] * 3 + 
                     severity_counts['severe'] * 5)
        max_risk_score = self.strange_response_count * 5 if self.strange_response_count > 0 else 1
        risk_percentage = (risk_score / max_risk_score * 100) if max_risk_score > 0 else 0
        
        # ëª¨ë°”ì¼ í™”ë©´ì— ìµœì í™”ëœ ì„¸ë¡œí˜• ë ˆì´ì•„ì›ƒ ìƒì„±
        fig = plt.figure(figsize=(9, 16), facecolor='#f8f9fa')  # ë” ê¹”ë”í•œ ë°°ê²½ìƒ‰
        
        # ì „ì²´ íƒ€ì´í‹€ ì¶”ê°€
        fig.suptitle('ğŸ§  ì¹˜ë§¤ ì§„ë‹¨ ëŒ€í™” ë¶„ì„ ë¦¬í¬íŠ¸', fontsize=18, fontweight='bold', y=0.98, color='#2c3e50')
        
        # 1. ìƒë‹¨: ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
        ax1 = plt.subplot2grid((6, 2), (0, 0), colspan=2, rowspan=1)
        try:
            img = Image.open(image_path)
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ëª¨ë°”ì¼ í™”ë©´ì— ë§ê²Œ)
            img.thumbnail((500, 300), Image.Resampling.LANCZOS)
            ax1.imshow(img)
            ax1.axis('off')
            # ì´ë¯¸ì§€ í…Œë‘ë¦¬ ì¶”ê°€
            for spine in ax1.spines.values():
                spine.set_visible(True)
                spine.set_linewidth(2)
                spine.set_color('#34495e')
        except Exception as e:
            ax1.text(0.5, 0.5, f'ğŸ“· ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨\n{os.path.basename(image_path)}', 
                    ha='center', va='center', fontsize=12, 
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="#e74c3c", alpha=0.8, edgecolor='none'))
            ax1.set_xlim(0, 1)
            ax1.set_ylim(0, 1)
            ax1.axis('off')
        
        # 2. ì™¼ìª½: ì£¼ìš” ìˆ˜ì¹˜ í‘œì‹œ
        ax2 = plt.subplot2grid((6, 2), (1, 0), rowspan=2)
        ax2.axis('off')
        
        # ìˆ˜ì¹˜ ì •ë³´ í…ìŠ¤íŠ¸ (ë” ê¹”ë”í•˜ê²Œ)
        stats_text = f"""ğŸ“Š ëŒ€í™” ë¶„ì„ ê²°ê³¼

â–ª ì „ì²´ ë‹µë³€: {total_responses}íšŒ
â–ª ì´ìƒ ë‹µë³€: {self.strange_response_count}íšŒ ({(self.strange_response_count/total_responses*100):.1f}%)
â–ª ìœ„í—˜ë„: {risk_percentage:.1f}% ({risk_score}/{max_risk_score}ì )

ì‹¬ê°ë„ë³„ ë¶„ë¥˜:
  ğŸŸ¡ ê²½ë¯¸: {severity_counts['mild']}íšŒ
  ğŸŸ  ë³´í†µ: {severity_counts['moderate']}íšŒ  
  ğŸ”´ ì‹¬ê°: {severity_counts['severe']}íšŒ"""
        
        ax2.text(0.05, 0.95, stats_text, fontsize=12, va='top', 
                bbox=dict(boxstyle="round,pad=0.8", facecolor="#ecf0f1", alpha=0.9, 
                         edgecolor='#bdc3c7', linewidth=1.5))
        
        # 3. ì˜¤ë¥¸ìª½: ìƒì„¸ ê¸°ë¡ ì˜ˆì‹œ
        ax3 = plt.subplot2grid((6, 2), (1, 1), rowspan=2)
        ax3.axis('off')
        
        # ìƒì„¸ ê¸°ë¡ ì˜ˆì‹œ í…ìŠ¤íŠ¸
        detail_examples = "ğŸ“ ìƒì„¸ ê¸°ë¡ ì˜ˆì‹œ\n\n"
        
        if self.strange_response_count > 0:
            # ìµœëŒ€ 3ê°œ ì˜ˆì‹œë§Œ í‘œì‹œ
            examples_to_show = min(3, len(self.strange_responses))
            for i, response in enumerate(self.strange_responses[:examples_to_show]):
                severity_emoji = {"mild": "ğŸŸ¡", "moderate": "ğŸŸ ", "severe": "ğŸ”´"}
                detail_examples += f"{severity_emoji[response.severity]} [{response.severity.upper()}]\n"
                detail_examples += f"Q: {response.question[:25]}...\n"
                detail_examples += f"A: {response.answer[:25]}...\n\n"
            
            if len(self.strange_responses) > 3:
                detail_examples += f"... ì™¸ {len(self.strange_responses) - 3}ê±´ ë”"
        else:
            detail_examples += "âœ… ì´ìƒ ë‹µë³€ì´ ê°ì§€ë˜ì§€\n    ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nì •ìƒì ì¸ ëŒ€í™”ê°€\nì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        ax3.text(0.05, 0.95, detail_examples, fontsize=11, va='top',
                bbox=dict(boxstyle="round,pad=0.8", facecolor="#fff3cd" if self.strange_response_count > 0 else "#d4edda", 
                         alpha=0.9, edgecolor='#ffeaa7' if self.strange_response_count > 0 else '#c3e6cb', linewidth=1.5))
        
        # 4. ì™¼ìª½ í•˜ë‹¨: ì „ì²´ ëŒ€í™” ë¶„ì„ ë°” ê·¸ë˜í”„
        ax4 = plt.subplot2grid((6, 2), (3, 0), rowspan=2)
        
        categories = ['ì •ìƒ\në‹µë³€', 'ì´ìƒ\në‹µë³€']
        counts = [total_responses - self.strange_response_count, self.strange_response_count]
        colors = ['#27ae60', '#e74c3c']  # ë” ì„ ëª…í•œ ìƒ‰ìƒ
        
        bars1 = ax4.bar(categories, counts, color=colors, alpha=0.8, width=0.6, edgecolor='white', linewidth=2)
        ax4.set_title('ğŸ’¬ ì „ì²´ ëŒ€í™” ë¶„ì„', fontsize=13, fontweight='bold', pad=15, color='#2c3e50')
        ax4.set_ylabel('ë‹µë³€ íšŸìˆ˜', fontsize=11, color='#34495e')
        
        # ë°” ìœ„ì— ìˆ«ì í‘œì‹œ (ë” ê¹”ë”í•˜ê²Œ)
        for bar, count in zip(bars1, counts):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.2,
                    f'{count}íšŒ', ha='center', va='bottom', fontsize=11, fontweight='bold', color='#2c3e50')
        
        # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ë§
        ax4.set_ylim(0, max(counts) * 1.3)
        ax4.grid(axis='y', alpha=0.2, linestyle='--')
        ax4.spines['top'].set_visible(False)
        ax4.spines['right'].set_visible(False)
        ax4.tick_params(colors='#34495e')
        
        # 5. ì˜¤ë¥¸ìª½ í•˜ë‹¨: ì‹¬ê°ë„ë³„ ì´ìƒ ë‹µë³€ ë°” ê·¸ë˜í”„
        ax5 = plt.subplot2grid((6, 2), (3, 1), rowspan=2)
        
        if self.strange_response_count > 0:
            severity_labels = ['ê²½ë¯¸', 'ë³´í†µ', 'ì‹¬ê°']
            severity_values = [severity_counts['mild'], severity_counts['moderate'], severity_counts['severe']]
            severity_colors = ['#f39c12', '#e67e22', '#e74c3c']  # ë” ì„ ëª…í•œ ìƒ‰ìƒ
            
            bars2 = ax5.bar(severity_labels, severity_values, color=severity_colors, alpha=0.8, 
                           width=0.6, edgecolor='white', linewidth=2)
            ax5.set_title('âš ï¸ ì´ìƒ ë‹µë³€ ì‹¬ê°ë„', fontsize=13, fontweight='bold', pad=15, color='#2c3e50')
            ax5.set_ylabel('ë‹µë³€ íšŸìˆ˜', fontsize=11, color='#34495e')
            
            # ë°” ìœ„ì— ìˆ«ì í‘œì‹œ
            for bar, count in zip(bars2, severity_values):
                height = bar.get_height()
                if height > 0:
                    ax5.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                            f'{count}íšŒ', ha='center', va='bottom', fontsize=11, fontweight='bold', color='#2c3e50')
            
            ax5.set_ylim(0, max(severity_values) * 1.4 if max(severity_values) > 0 else 1)
            ax5.grid(axis='y', alpha=0.2, linestyle='--')
            ax5.spines['top'].set_visible(False)
            ax5.spines['right'].set_visible(False)
            ax5.tick_params(colors='#34495e')
        else:
            ax5.text(0.5, 0.5, 'âœ… ì´ìƒ ë‹µë³€ ì—†ìŒ', ha='center', va='center', 
                    fontsize=13, fontweight='bold', color='#27ae60', transform=ax5.transAxes,
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="#d4edda", alpha=0.8, edgecolor='#c3e6cb'))
            ax5.set_xlim(0, 1)
            ax5.set_ylim(0, 1)
            ax5.axis('off')
        
        # ì „ì²´ ë ˆì´ì•„ì›ƒ ì¡°ì •
        plt.tight_layout(rect=[0, 0.08, 1, 0.95], pad=2.0)
        
        # 6. í•˜ë‹¨: ê¶Œì¥ì‚¬í•­ (ë” ëˆˆì— ë„ê²Œ)
        if severity_counts['severe'] >= 2 or risk_percentage > 80:
            recommendation = "ğŸš¨ ì „ë¬¸ì˜ ìƒë‹´ ì‹œê¸‰"
            rec_color = '#e74c3c'
            bg_color = '#fadbd8'
        elif severity_counts['severe'] >= 1 or risk_percentage > 60:
            recommendation = "âš ï¸ ì£¼ì˜ ê´€ì°° í•„ìš”"
            rec_color = '#e67e22'
            bg_color = '#fdeaa7'
        elif risk_percentage > 40:
            recommendation = "ğŸ’› ì •ê¸°ì  ê´€ì°° ê¶Œì¥"
            rec_color = '#f39c12'
            bg_color = '#fcf3cf'
        else:
            recommendation = "âœ… ì–‘í˜¸í•œ ìƒíƒœ"
            rec_color = '#27ae60'
            bg_color = '#d5f4e6'
        
        # ê¶Œì¥ì‚¬í•­ ë°•ìŠ¤ (ë” í¬ê³  ëˆˆì— ë„ê²Œ)
        fig.text(0.5, 0.04, recommendation, ha='center', va='center', 
                fontsize=16, fontweight='bold', color=rec_color,
                bbox=dict(boxstyle="round,pad=1.0", facecolor=bg_color, alpha=0.9, 
                         edgecolor=rec_color, linewidth=2))
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        image_basename = os.path.splitext(os.path.basename(image_path))[0]
        report_filename = os.path.join(output_dir, f"{image_basename}_mobile_report_{timestamp}.png")
        
        # ê³ í•´ìƒë„ë¡œ ì €ì¥ (ëª¨ë°”ì¼ í™”ë©´ìš©)
        plt.savefig(report_filename, dpi=200, bbox_inches='tight', 
                    facecolor='#f8f9fa', edgecolor='none', format='png')
        plt.close()
        
        print(f"ğŸ“± ëª¨ë°”ì¼ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_filename}")
        return report_filename
    
    def get_conversation_summary(self):
        """ëŒ€í™” ì¢…ë£Œ í›„ ì´ìƒí•œ ë‹µë³€ ìš”ì•½ ì œê³µ"""
        # ì „ì²´ ë‹µë³€ íšŸìˆ˜ ê³„ì‚°
        total_responses = len(self.conversation_turns)
        
        if total_responses == 0:
            return "ëŒ€í™”ê°€ ì§„í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        if self.strange_response_count == 0:
            return f"ğŸ‰ ëŒ€í™” ì¤‘ íŠ¹ë³„íˆ ì´ìƒí•œ ë‹µë³€ì€ ì—†ì—ˆìŠµë‹ˆë‹¤. ì¢‹ì€ ëŒ€í™”ì˜€ì–´ìš”!\nì „ì²´ ë‹µë³€ íšŸìˆ˜: {total_responses}íšŒ"
        
        summary = f"\n{'='*50}\n"
        summary += f"ğŸ“Š ëŒ€í™” ì¢…ë£Œ - ë¶„ì„ ê²°ê³¼\n"
        summary += f"{'='*50}\n"
        summary += f"ğŸ“Œ ì „ì²´ ë‹µë³€ íšŸìˆ˜: {total_responses}íšŒ\n"
        summary += f"ğŸ” ì´ìƒí•œ ë‹µë³€ íšŸìˆ˜: {self.strange_response_count}íšŒ ({(self.strange_response_count/total_responses*100):.1f}%)\n\n"
        
        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        severity_counts = {"mild": 0, "moderate": 0, "severe": 0}
        for response in self.strange_responses:
            severity_counts[response.severity] += 1
        
        summary += f"ì´ìƒí•œ ë‹µë³€ ì¤‘ ì‹¬ê°ë„ë³„ ë¶„ë¥˜:\n"
        summary += f"  â€¢ ê²½ë¯¸ (Mild): {severity_counts['mild']}íšŒ ({(severity_counts['mild']/self.strange_response_count*100):.1f}%)\n"
        summary += f"  â€¢ ë³´í†µ (Moderate): {severity_counts['moderate']}íšŒ ({(severity_counts['moderate']/self.strange_response_count*100):.1f}%)\n"
        summary += f"  â€¢ ì‹¬ê° (Severe): {severity_counts['severe']}íšŒ ({(severity_counts['severe']/self.strange_response_count*100):.1f}%)\n\n"
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
        # ê²½ë¯¸: 1ì , ë³´í†µ: 3ì , ì‹¬ê°: 5ì 
        risk_score = (severity_counts['mild'] * 1 + 
                     severity_counts['moderate'] * 3 + 
                     severity_counts['severe'] * 5)
        
        # ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜ ê³„ì‚° (ëª¨ë“  ì´ìƒí•œ ë‹µë³€ì´ severeì¸ ê²½ìš°)
        max_risk_score = self.strange_response_count * 5
        
        # ìœ„í—˜ë„ í¼ì„¼íŠ¸ ê³„ì‚°
        risk_percentage = (risk_score / max_risk_score * 100)
        
        summary += f"ìœ„í—˜ë„ ì ìˆ˜: {risk_score}ì  / {max_risk_score}ì  ({risk_percentage:.1f}%)\n"
        summary += f"   (ê²½ë¯¸=1ì , ë³´í†µ=3ì , ì‹¬ê°=5ì  ê°€ì¤‘ì¹˜ ì ìš©)\n\n"
        
        summary += f"ìƒì„¸ ê¸°ë¡:\n"
        for i, response in enumerate(self.strange_responses, 1):
            summary += f"\n{i}. [{response.severity.upper()}] {response.timestamp}\n"
            summary += f"   ì§ˆë¬¸: {response.question[:100]}{'...' if len(response.question) > 100 else ''}\n"
            summary += f"   ë‹µë³€: {response.answer[:100]}{'...' if len(response.answer) > 100 else ''}\n"
        
        # ê¶Œì¥ì‚¬í•­ - ìœ„í—˜ë„ ì ìˆ˜ì™€ ì‹¬ê° ë‹µë³€ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
        severe_percent = (severity_counts['severe'] / total_responses * 100)
        
        if severity_counts['severe'] >= 2 or risk_percentage > 80:
            summary += f"\n  ê¶Œì¥ì‚¬í•­: ì‹¬ê°í•œ ìˆ˜ì¤€ì˜ ì´ìƒ ë‹µë³€ì´ {severity_counts['severe']}íšŒ ê´€ì°°ë˜ì—ˆìœ¼ë©°, "
            summary += f"ì´ìƒ ë‹µë³€ì˜ ìœ„í—˜ë„ê°€ {risk_percentage:.1f}%ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. í•œë²ˆ ì–´ë¥´ì‹  ë¶„ì„ ì°¾ì•„ ëµ™ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?\n"
        elif severity_counts['severe'] >= 1 or risk_percentage > 60:
            summary += f"\n ê¶Œì¥ì‚¬í•­: ì‹¬ê°í•œ ë‹µë³€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ì´ìƒ ë‹µë³€ì˜ ìœ„í—˜ë„ê°€ {risk_percentage:.1f}%ì…ë‹ˆë‹¤. "
            summary += f"ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
        elif risk_percentage > 40:
            summary += f"\n ê¶Œì¥ì‚¬í•­: ì´ìƒ ë‹µë³€ì˜ ìœ„í—˜ë„ê°€ {risk_percentage:.1f}%ë¡œ ì¤‘ê°„ ìˆ˜ì¤€ì…ë‹ˆë‹¤. "
            summary += f"ì–´ë¥´ì‹ ì—ê²Œ ì „í™” í•œí†µ ê±¸ì–´ ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?\n"
        elif risk_percentage > 20:
            summary += f"\n ê¶Œì¥ì‚¬í•­: ì´ìƒ ë‹µë³€ì˜ ìœ„í—˜ë„ê°€ {risk_percentage:.1f}%ë¡œ ê²½ë¯¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤. "
            summary += f"ì£¼ê¸°ì ì¸ ê´€ì°°ì„ ê³„ì†í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"
        else:
            summary += f"\n ì´ìƒ ë‹µë³€ì˜ ìœ„í—˜ë„ê°€ {risk_percentage:.1f}%ë¡œ ë‚®ì€ ìˆ˜ì¤€ì…ë‹ˆë‹¤. "
            summary += f"í˜„ì¬ ìƒíƒœë¥¼ ì˜ ìœ ì§€í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"
        
        # ì¶”ê°€ ì•ˆë‚´ì‚¬í•­
        if severity_counts['severe'] > 0:
            summary += f"\n ì°¸ê³ : ì‹¬ê°í•œ ë‹µë³€ì€ ì‹œê³µê°„ ì§€ë‚¨ë ¥ ìƒì‹¤, ì™„ì „í•œ ë§¥ë½ ì´íƒˆ ë“±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n"
        
        summary += f"{'='*50}\n"
        
        return summary
    
    def generate_story_from_conversation(self, image_path):
        """ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì¸ë¶„ì˜ ê´€ì ì—ì„œ ìŠ¤í† ë¦¬ ìƒì„±"""
        # ëŒ€í™” ë‚´ìš© ì •ë¦¬
        conversation_text = ""
        for turn in self.conversation_turns:
            conversation_text += f"ì§ˆë¬¸: {turn.question}\në‹µë³€: {turn.answer}\n\n"
        
        # ìŠ¤í† ë¦¬ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        story_prompt = f"""
ë‹¤ìŒì€ í•œ ì–´ë¥´ì‹ ì´ ì˜›ë‚  ì‚¬ì§„ì„ ë³´ë©° ë‚˜ëˆˆ ëŒ€í™”ì…ë‹ˆë‹¤:

{conversation_text}

ì´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ì§„ ì† ìˆœê°„ì— ëŒ€í•œ ì–´ë¥´ì‹ ì˜ ì¶”ì–µì„ 1ì¸ì¹­ ì‹œì ìœ¼ë¡œ 10ì¤„ ì •ë„ì˜ ì§§ì€ ì´ì•¼ê¸°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‘ì„± ì§€ì¹¨:
1. ì–´ë¥´ì‹ ì˜ ê°ì •ê³¼ ë‹¹ì‹œì˜ ëŠë‚Œì„ ìƒìƒí•˜ê²Œ í‘œí˜„
2. êµ¬ì²´ì ì¸ ê°ê°ì  ë¬˜ì‚¬ í¬í•¨ (ì†Œë¦¬, ëƒ„ìƒˆ, ì´‰ê° ë“±)
3. ë”°ëœ»í•˜ê³  í–¥ìˆ˜ë¥¼ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ëŠ” í†¤
4. ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
5. ë§ˆì¹˜ ì†ì/ì†ë…€ì—ê²Œ ë“¤ë ¤ì£¼ëŠ” ê²ƒì²˜ëŸ¼ ì¹œê·¼í•œ ì–´íˆ¬

ìŠ¤í† ë¦¬ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.deployment,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë…¸ì¸ì˜ ì¶”ì–µì„ ì•„ë¦„ë‹µê²Œ ì¬êµ¬ì„±í•˜ëŠ” ìŠ¤í† ë¦¬í…”ëŸ¬ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": story_prompt}
                ],
                max_tokens=1024,
                temperature=0.8,
                top_p=1.0,
            )
            
            story = response.choices[0].message.content
            
            # story_telling í´ë” ìƒì„±
            story_dir = "story_telling"
            os.makedirs(story_dir, exist_ok=True)
            
            # ì´ë¯¸ì§€ íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ì—¬ ìŠ¤í† ë¦¬ íŒŒì¼ëª… ìƒì„±
            image_basename = os.path.splitext(os.path.basename(image_path))[0]
            story_filename = os.path.join(story_dir, f"{image_basename}.txt")
            
            # ìŠ¤í† ë¦¬ íŒŒì¼ ì €ì¥
            with open(story_filename, 'w', encoding='utf-8') as f:
                f.write(story)
            
            print(f"ì¶”ì–µ ì´ì•¼ê¸°ê°€ '{story_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return story, story_filename
            
        except Exception as e:
            print(f"ìŠ¤í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None
    
    def save_conversation_to_file(self, filename_prefix="conversation", image_path=None):
        """ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì´ë¯¸ì§€ íŒŒì¼ëª… ì¶”ì¶œ (í™•ì¥ì ì œì™¸)
        if image_path:
            image_basename = os.path.splitext(os.path.basename(image_path))[0]
            base_filename = f"{image_basename}_{timestamp}"
        else:
            base_filename = f"{filename_prefix}_{timestamp}"
        
        # í´ë” ìƒì„± (ì—†ëŠ” ê²½ìš°)
        conversation_dir = "conversation_log"
        analysis_dir = "analysis"
        os.makedirs(conversation_dir, exist_ok=True)
        os.makedirs(analysis_dir, exist_ok=True)
        
        # ëŒ€í™” ê¸°ë¡ íŒŒì¼ ì €ì¥
        conversation_filename = os.path.join(conversation_dir, f"{base_filename}.txt")
        with open(conversation_filename, 'w', encoding='utf-8') as f:
            f.write(f"=== ëŒ€í™” ê¸°ë¡ ===\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
            f.write(f"{'='*50}\n\n")
            
            for i, turn in enumerate(self.conversation_turns, 1):
                f.write(f"[ëŒ€í™” {i}] {turn.timestamp}\n")
                f.write(f"ì§ˆë¬¸: {turn.question}\n")
                f.write(f"ë‹µë³€: {turn.answer}\n")
                f.write(f"{'-'*30}\n\n")
        
        print(f"ëŒ€í™” ê¸°ë¡ì´ '{conversation_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì´ìƒ ë‹µë³€ ë¶„ì„ íŒŒì¼ ì €ì¥
        analysis_filename = None
        if self.strange_response_count > 0:
            analysis_filename = os.path.join(analysis_dir, f"{base_filename}_analysis.txt")
            with open(analysis_filename, 'w', encoding='utf-8') as f:
                f.write(self.get_conversation_summary())
            
            print(f"ì´ìƒ ë‹µë³€ ë¶„ì„ì´ '{analysis_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return conversation_filename, analysis_filename


def analyze_and_describe_image(image_path, user_description="", user_description_date=""):
    """ì´ë¯¸ì§€ ë¶„ì„ ë° ì„¤ëª… í†µí•© í•¨ìˆ˜"""
    # 1. GPT-4o ì´ë¯¸ì§€ ë¶„ì„ ê°ì²´ ìƒì„±
    analyzer = ImageAnalysisGPT()
    
    # 2. ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰
    print(f"GPT-4oë¡œ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘: {image_path}")
    analysis_result = analyzer.analyze_image_with_gpt(image_path)
    
    if not analysis_result:
        return "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", None, None
    
    # 3. LLM ëŒ€í™” ê°ì²´ ìƒì„±
    llm_chat = LLMDescription()
    
    # 4. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
    print("\nëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì¤‘...")
    llm_chat.setup_conversation_context(
        analysis_result, 
        user_description, 
        user_description_date
    )
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n===== GPT-4o ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ =====")
    print("ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    return "ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ", llm_chat, image_path


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # ë¶„ì„í•  ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥
    image_path = "images.jpg" #DBì—ì„œ ê°€ì ¸ì˜¨ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(image_path):
        print(f"ì˜¤ë¥˜: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - {image_path}")
        exit(1)
    
    # user_description_date = input("ì‚¬ì§„ì˜ ë‚ ì§œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 1980ë…„ëŒ€, 2000ë…„ ì—¬ë¦„): ")
    # user_description = input("ì‚¬ì§„ì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ")  --> ë‘ë¶€ë¶„ë„ DBì—ì„œ ê°€ì ¸ì™€ì•¼í•¨
    
    # ì´ë¯¸ì§€ ë¶„ì„ ë° ëŒ€í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    result, llm_chat, img_path = analyze_and_describe_image(
        image_path
        # user_description, 
        # user_description_date
    )
    
    if llm_chat:
        print("\n===== ì´ë¯¸ì§€ì— ê´€í•œ ëŒ€í™” ì‹œì‘ =====")
        
        # GPTê°€ ë¨¼ì € ì§ˆë¬¸ ë˜ì§€ê¸°
        initial_question = llm_chat.generate_initial_question()
        print(f"\nAI: {initial_question}")
        
        print("\nğŸ’¡ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        while True:
            user_query = input("\në‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”: ")
            
            # ìˆ˜ë™ ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if user_query.lower() in ['exit', 'ì¢…ë£Œ', 'quit', 'q']:
                print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # GPTì—ê²Œ ì‚¬ìš©ì ë‹µë³€ ì „ë‹¬ ë° í† í° ì œí•œ í™•ì¸
            answer, should_end = llm_chat.chat_about_image(user_query)
            print(f"\nAI: {answer}")
            
            # í† í° ì œí•œìœ¼ë¡œ ì¸í•œ ì¢…ë£Œ í™•ì¸
            if should_end:
                print("\nëŒ€í™” í† í° ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
        
        # ëŒ€í™” ì¢…ë£Œ í›„ íŒŒì¼ë¡œ ì €ì¥
        llm_chat.save_conversation_to_file(image_path=img_path)
        
        # ì¶”ì–µ ìŠ¤í† ë¦¬ ìƒì„±
        print("\nì¶”ì–µ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
        story, story_file = llm_chat.generate_story_from_conversation(img_path)
        
        if story:
            print(f"\n=== ìƒì„±ëœ ì¶”ì–µ ì´ì•¼ê¸° ===\n{story}\n")
        
        # ğŸ“± ëª¨ë°”ì¼ ë¦¬í¬íŠ¸ ìƒì„± (ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ëŠ¥)
        print("\nğŸ“± ëª¨ë°”ì¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
        mobile_report_file = llm_chat.generate_mobile_report(img_path)
        
        if mobile_report_file:
            print(f"âœ… ëª¨ë°”ì¼ ë¦¬í¬íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“‚ íŒŒì¼ ê²½ë¡œ: {mobile_report_file}")
        
        # ì½˜ì†”ì—ë„ ìš”ì•½ ì¶œë ¥
        print(llm_chat.get_conversation_summary())
        
    else:
        print("ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í•˜ì—¬ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")