from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime
from fastapi import HTTPException
from azure.storage.blob import BlobServiceClient
from uuid import uuid4

from services.image_analyzer import ImageAnalyzer
from services.chat_system import ChatSystem
from services.voice_system import VoiceSystem
from services.story_and_report_system import StoryGenerator
import os
import uuid

from core.config import settings
from db.database import get_db

@dataclass
class SessionData:
    conversation_id: str
    photo_path: Optional[str] = None
    turns: List[dict] = field(default_factory=list)  # {"question": ..., "answer": ..., "timestamp": ...}
    created_at: datetime = field(default_factory=datetime.now)

class OptimizedDementiaSystem:
    """ìµœì í™”ëœ ì¹˜ë§¤ ì§„ë‹¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.sessions: dict[str, SessionData] = {}  # ì—¬ê¸° keyê°€ conv_id
        self.speech_key = os.getenv("AZURE_SPEECH_KEY")

        self.image_analyzer = ImageAnalyzer()
        self.chat_system = ChatSystem()
        self.voice_system = VoiceSystem() if self.speech_key else None
        self.story_generator = StoryGenerator(self.chat_system)
    
    def analyze_and_start_conversation(self, image_path):
        """ì´ë¯¸ì§€ ë¶„ì„ ë° ëŒ€í™” ì‹œì‘"""
        if not os.path.exists(image_path):
            return None
        

        # ì´ë¯¸ì§€ ë¶„ì„
        analysis_result = self.image_analyzer.analyze_image(image_path)
        if not analysis_result:
            return None
        
        # ëŒ€í™” ì„¤ì •
        self.chat_system.setup_conversation_context(analysis_result)
    
        # ì²« ì§ˆë¬¸ ìƒì„±
        initial_question = self.chat_system.generate_initial_question()

        # ì²« ì§ˆë¬¸ TTS
        audio_path = self.voice_system.synthesize_speech(initial_question)
        
        return initial_question, audio_path
    
    def generate_complete_analysis(self, image_path):
        """ì™„ì „í•œ ë¶„ì„ ìƒì„±"""
        print("\nğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼ ìƒì„± ì¤‘...")
        
        # 1. ëŒ€í™” ê¸°ë¡ ì €ì¥ (ìƒˆë¡œìš´ í´ë” êµ¬ì¡°)
        conversation_file, analysis_file = self.story_generator.save_conversation_to_file(image_path)
        
        # 2. ì¶”ì–µ ìŠ¤í† ë¦¬ ìƒì„±
        story, story_file = self.story_generator.generate_story_from_conversation(image_path)
        
        # 3. ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
        summary = self.story_generator.save_conversation_summary()
        print(summary)
        
        # 4. ìŠ¤í† ë¦¬ ì¶œë ¥
        if story:
            print(f"\n{'='*50}")
            print("ğŸ“– ìƒì„±ëœ ì¶”ì–µ ì´ì•¼ê¸°")
            print(f"{'='*50}")
            print(story)
            print(f"{'='*50}")
        
        return {
            'conversation_file': conversation_file,
            'analysis_file': analysis_file,
            'story_file': story_file,
            'story_content': story,
            'summary': summary,
            'conversation_id': self.story_generator.conversation_id
        }
    
    def _run_conversation(self, initial_question, is_voice=False):
        """ëŒ€í™” ë£¨í”„ ì‹¤í–‰ (ìŒì„±/í…ìŠ¤íŠ¸ ê³µí†µ)"""
        
        # if is_voice and self.voice_system:
        #     welcome_msg = "ì•ˆë…•í•˜ì„¸ìš”. ì‚¬ì§„ì„ ë³´ë©° ëŒ€í™”í•´ìš”."
        #     print(f"ğŸ¤– {welcome_msg}")
        #     self.voice_system.synthesize_speech(welcome_msg)
            
        #     print(f"ğŸ¤– {initial_question}")
        #     self.voice_system.synthesize_speech(initial_question)
        # else:
        #     print(f"ğŸ¤– {initial_question}")
        
        # conversation_type = "ìŒì„±" if is_voice else "í…ìŠ¤íŠ¸"
        # print(f"\n" + "="*40)
        # print(f"{'ğŸ™ï¸' if is_voice else 'ğŸ’¬'} {conversation_type} ëŒ€í™” ì‹œì‘!")
        # print(f"ğŸ’¡ {'ì¢…ë£Œë¼ê³  ë§í•˜ë©´' if is_voice else 'exit ë˜ëŠ” ì¢…ë£Œë¥¼ ì…ë ¥í•˜ë©´'} ëë‚©ë‹ˆë‹¤")
        # print("="*40)
        
        # ëŒ€ë‹µ
        should_end = False
        if is_voice and self.voice_system:
            print("ğŸ™ï¸ ë§ì”€í•´ ì£¼ì„¸ìš”...")
            # ìŒì„± ë…¹ìŒ ì‹œì‘
            self.chat_system.start_recording()
            user_input = self.voice_system.transcribe_speech()
            # ìŒì„± ë…¹ìŒ ì¤‘ì§€
            audio_path = self.chat_system.stop_recording()
            
            if user_input == "ì¢…ë£Œ":
                end_msg = "ëŒ€í™”ë¥¼ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."
                print(f"ğŸ¤– {end_msg}")
                self.voice_system.synthesize_speech(end_msg)
                should_end = True
        else:
            user_input = input("\nğŸ‘¤ ë‹µë³€: ").strip()
            if user_input.lower() in ['exit', 'ì¢…ë£Œ', 'quit', 'q']:
                print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                should_end = True

        return user_input, audio_path, should_end

        # # AI ì‘ë‹µ (ìŒì„± ëª¨ë“œì¼ ë•ŒëŠ” ë…¹ìŒëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ ì „ë‹¬)
        # answer, should_end = self.chat_system.chat_about_image2(user_input, with_audio=is_voice)
        # print(f"ğŸ¤– {answer}")
        
        # if is_voice and self.voice_system:
        #     self.voice_system.synthesize_speech(answer)
        
        # if should_end:
        #     end_msg = "ëŒ€í™” ì‹œê°„ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        #     print(f"â° {end_msg}")
        #     if is_voice and self.voice_system:
        #         self.voice_system.synthesize_speech(end_msg)
        
        # # ì¢…í•© ë¶„ì„ ìƒì„±
        # analysis_results = self.generate_complete_analysis(image_path)
        
        # if analysis_results['conversation_file']:
        #     print(f"ğŸ“‚ ëŒ€í™”ê¸°ë¡: {analysis_results['conversation_file']}")
        #     print(f"ğŸ“Š ë¶„ì„ê²°ê³¼: {analysis_results['analysis_file']}")
        #     if analysis_results['story_file']:
        #         print(f"ğŸ“– ìŠ¤í† ë¦¬: {analysis_results['story_file']}")
        # return analysis_results
    
    def one_chat_about_image(self, user_query, with_audio=False):
        """ëŒ€í™” ì²˜ë¦¬"""
        user_tokens = len(self.tokenizer.encode(user_query))
        
        # ìŒì„± ë…¹ìŒ ì‹œì‘ (if requested)
        audio_file = None
        if with_audio:
            self.start_recording()
        
        # ëŒ€í™” í„´ ì €ì¥
        if self.last_question:
            # ìŒì„± ë…¹ìŒ ì¤‘ì§€ ë° íŒŒì¼ ì €ì¥ (if recording)
            if with_audio:
                audio_file = self.stop_recording()
            
            # conversation_turn = ConversationTurn(
            #     question=self.last_question,
            #     answer=user_query,
            #     timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            #     answer_length=len(user_query.strip()),
            #     audio_file=audio_file if audio_file else ""
            # )
            # self.conversation_turns.append(conversation_turn)
            
        
        self.conversation_history.append({"role": "user", "content": user_query})
        self.token_count += user_tokens
        
        # í† í° ì œí•œ í™•ì¸
        if self.token_count > self.max_tokens:
            answer = "ëŒ€í™” ì‹œê°„ì´ ë‹¤ ë˜ì—ˆì–´ìš”. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤."
            self.conversation_history.append({"role": "assistant", "content": answer})
            return answer, True
        
        # AI ì‘ë‹µ ìƒì„±
        response = self.client.chat.completions.create(
            model=self.deployment,
            messages=self.conversation_history,
            max_tokens=1024,
            temperature=0.7
        )
        answer = response.choices[0].message.content
        
        self.conversation_history.append({"role": "assistant", "content": answer})
        self.token_count += len(self.tokenizer.encode(answer))
        self.last_question = answer
        
        if self.token_count > self.max_tokens:
            return answer, True
        
        return answer, False
    
    def voice_conversation(self, image_path):
        """ìŒì„± ëŒ€í™” ì‹¤í–‰"""
        if not self.voice_system:
            return None
        return self._run_conversation_loop(image_path, is_voice=True)
    
    def text_conversation(self, image_path):
        """í…ìŠ¤íŠ¸ ëŒ€í™” ì‹¤í–‰"""
        return self._run_conversation_loop(image_path, is_voice=False)


async def upload_audio_to_blob(audio_path: str) -> str:
    """
    Uploads a wav audio file to Azure Blob Storage and returns the URL.
    """
    storage_key = os.getenv("AZURE_BLOBSTORAGE_KEY")
    storage_account = os.getenv("AZURE_BLOBSTORAGE_ACCOUNT")
    container_name = "talking-voice"
    AZURE_STORAGE_CONNECTION_STRING=f"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_key};EndpointSuffix=core.windows.net"

    try:
        # Replace this with your actual Azure Blob Storage connection string
        connect_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
        blob_service_client = BlobServiceClient.from_connection_string(connect_str)

        original_filename = os.path.basename(audio_path)

        # Container name
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=f"{uuid4()}_{original_filename}")

        # Upload the file
        with open(audio_path, "rb") as data:
            blob_client.upload_blob(data, overwrite=True)

        return blob_client.url

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Azure Blob upload failed: {str(e)}")


# async def upload_audio_to_blob(file_path: str, original_filename: str, blob_service_client) -> str:
#     """
#     Azure Blob Storageì— wav ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì£¼ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     blob_name = f"{uuid.uuid4()}_{original_filename}"
#     try:
#         # BlobStorageService ì¸ìŠ¤í„´ìŠ¤ì¼ ê²½ìš°
#         if hasattr(blob_service_client, 'container_client'):
#             blob_client = blob_service_client.container_client.get_blob_client(blob_name)
#             with open(file_path, "rb") as data:
#                 blob_client.upload_blob(data, overwrite=True)
#                 return blob_client.url
#         else:
#             # (ê¸°ì¡´ ë¹„ë™ê¸° BlobServiceClient ì‚¬ìš© ì¼€ì´ìŠ¤ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€)
#             raise Exception('ì§€ì›í•˜ì§€ ì•ŠëŠ” blob_service_client íƒ€ì…ì…ë‹ˆë‹¤.')
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"Blob Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# async def save_audio_to_db(photo_data: PhotoCreate, db: AsyncSession) -> Photo:
#     """
#     ì‚¬ì§„ ë©”íƒ€ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
#     """
#     try:
#         photo = Photo(
#             name=photo_data.name,
#             url=photo_data.url,
#             year=photo_data.year,
#             season=photo_data.season,
#             description=photo_data.description,
#             summary_text=photo_data.summary_text,
#             summary_voice=photo_data.summary_voice,
#             family_id=photo_data.family_id,
#             uploaded_at=datetime.now(timezone(timedelta(hours=9))).replace(tzinfo=None)
#         )
        
#         db.add(photo)
#         await db.commit()
#         await db.refresh(photo)
#         return photo
#     except Exception as e:
#         await db.rollback()
#         raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")